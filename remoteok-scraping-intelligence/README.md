# Remote Job Market Intelligence using Ethical Web Scraping

## Internship Mini Project  
*Organization:* Evoastra Ventures (OPC) Pvt Ltd  
*Project Type:* Data Science Internship Project  
*Difficulty Level:* Beginner-Friendly  
*Target Website:* https://remoteok.com  

---

## Project Overview
This project focuses on analyzing the remote job market by ethically collecting job listings from *Remote OK*.  
The objective is to extract meaningful insights about *job roles, skill demand, job types, and geographic distribution* using real-world job posting data while strictly adhering to legal and ethical web scraping standards.

The project follows an **industry-level data pipeline**, covering:
- Data Collection
- Data Cleaning
- Data Analysis
- Data Visualization
- Documentation & Reporting

---

## Data Source & Ethical Compliance
- *Website:* Remote OK (https://remoteok.com)
- *Scraping Type:* Educational (Non-commercial)

### Compliance Measures Followed
- âœ”ï¸ Followed `robots.txt` rules
- âœ”ï¸ Implemented mandatory **1-second crawl delay**
- âœ”ï¸ Avoided forbidden endpoints (`?action=get_jobs`)
- âœ”ï¸ No aggressive, parallel, or automated bot scraping
- âœ”ï¸ Raw scraped data **not published publicly**
- âœ”ï¸ Data used strictly for educational purposes

This ensures compliance with ethical scraping standards and Evoastra internship guidelines.


---

## Tools & Technologies
- Python 3.8+
- requests
- BeautifulSoup
- pandas
- matplotlib
- seaborn
- Google Colab / VS Code

---

## ğŸ”„ Project Workflow

### 1. Web Scraping
- Extracted:
  - Job Title
  - Company Name
  - Skills / Tags
  - Location
  - Job Type
  - Date Posted
  - Job URL
- Scraped only **public HTML pages**
- Rate-limited requests to respect server load

### 2. Data Cleaning
- Removed duplicate records
- Handled missing and null values
- Standardized text fields (lowercase, trimming)
- Cleaned skills and location columns
- Generated a clean, analysis-ready dataset

### 3. Data Analysis
- Skill frequency analysis
- Job title distribution
- Job type distribution
- Location-based analysis
- Company-wise job posting trends

### 4. Data Visualization
- Top demanded skills
- Most common job titles
- Job type distribution
- Skill frequency comparison

### 5. Documentation
- Well-structured folder hierarchy
- Clear and readable code
- Professional reporting of findings, limitations, and methodology

---

## ğŸ“Š Key Insights
- Technical roles dominate the remote job market
- **Python, SQL, AWS, and JavaScript** are among the most demanded skills
- Majority of jobs are fully remote
- Analysis is primarily **descriptive** due to text-based data

---

## âš ï¸ Limitations
- Data source limited to Remote OK
- Dataset contains mostly non-numeric, unstructured text fields
- Salary and experience data not available
- Advanced statistical or ML analysis is not applicable

---

## ğŸ‘¥ Team Contribution
- **Web Scraping:** Scraping Team  
- **Data Cleaning:** Data Cleaning Team  
- **Analysis & Visualization:** Analysis Team  
- **Documentation & Coordination:** Documentation Team  

---

## ğŸ“ Compliance Statement
This project strictly follows ethical web scraping practices and Evoastra internship guidelines.  
No proprietary, restricted, or private data was accessed or redistributed.

---

## ğŸ“‰ Data Limitations & Biases
The dataset was scraped exclusively from **Remote OK**, and therefore:
- Does not represent the entire global remote job market
- Reflects only the platformâ€™s audience and job categories
- Is subject to time-based and platform-specific biases

These limitations are clearly acknowledged in the analysis and reporting.

---

**Evoastra Ventures (OPC) Pvt Ltd**  

*Data Science Internship â€“ Mini Project* 
## ğŸ“ Project Structure

---

```text
remoteok-scraping-project/
â”‚
â”œâ”€â”€ README.md                  # Project overview & documentation
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ .gitignore                 # Excludes raw data & unnecessary files
â”œâ”€â”€ Team_B.ipynb               # Jupyter Notebook (scraping, cleaning, analysis & visuals)
â”‚
â”œâ”€â”€ src/                       # Source code
â”‚   â”œâ”€â”€ scraper.py             # Ethical web scraping logic
â”‚   â”œâ”€â”€ data_cleaner.py        # Data cleaning pipeline
â”‚   â””â”€â”€ analyzer.py            # Data analysis & visualization
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ cleaned/
â”‚       â””â”€â”€ remoteok_jobs_cleaned.csv   # Final cleaned dataset
â”‚
â”œâ”€â”€ visualizations/
â”‚   â”œâ”€â”€ top_skills.png
â”‚   â”œâ”€â”€ job_type_distribution.png
â”‚   â”œâ”€â”€ top_job_titles.png
â”‚   â””â”€â”€ skill_frequency_comparison.png
â”‚
â””â”€â”€ reports/
    â”œâ”€â”€ analysis_report.pdf    # Final project report
    â””â”€â”€ methodology.md         # Technical methodology


